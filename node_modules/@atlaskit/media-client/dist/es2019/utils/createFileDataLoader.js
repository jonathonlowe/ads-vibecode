import Dataloader from 'dataloader';
import { getRandomHex } from '@atlaskit/media-common';
export const MAX_BATCH_SIZE = 100;
const isBatchLoadingErrorResult = result => {
  return result.error instanceof Error;
};
const isResponseFileItem = fileItem => {
  return 'details' in fileItem;
};
const makeCacheKey = (id, collection) => collection ? `${id}-${collection}` : id;
export const getItemsFromKeys = (dataloaderKeys, fileItems) => {
  const itemsByKey = fileItems.reduce((prev, fileItem) => {
    const {
      id,
      collection
    } = fileItem;
    const key = makeCacheKey(id, collection);
    if (isBatchLoadingErrorResult(fileItem)) {
      prev[key] = fileItem.error;
    } else if (isResponseFileItem(fileItem)) {
      prev[key] = {
        ...fileItem.details,
        metadataTraceContext: fileItem.metadataTraceContext
      };
    } else {
      prev[key] = {
        id,
        collection,
        type: 'not-found',
        metadataTraceContext: fileItem.metadataTraceContext
      };
    }
    return prev;
  }, {});
  return dataloaderKeys.map(dataloaderKey => {
    const {
      id,
      collectionName
    } = dataloaderKey;
    const key = makeCacheKey(id, collectionName);
    return itemsByKey[key] || {
      id,
      type: 'not-found'
    };
  });
};
/**
 * Returns a function that, given Array<DataloaderKey>, resolves to an array of same length containing either DataloaderResult or Error.
 * Such contract is formalised by Dataloader 1.0, @see https://github.com/graphql/dataloader
 *
 * If an Error is resolved in the results, it must be at same position then their corresponding key:
 * - Dataloader will re-throw that Error when accessing/loading that particular key
 *
 * @param mediaStore instance of MediaStore
 */
export function createBatchLoadingFunc(mediaStore) {
  return async keys => {
    const nonCollectionName = '__media-single-file-collection__';
    const includeHashByCollection = keys.reduce((acc, key) => {
      const collectionName = key.collectionName || nonCollectionName;
      if (key.includeHashForDuplicateFiles) {
        acc[collectionName] = key.includeHashForDuplicateFiles;
      }
      return acc;
    }, {});
    const fileIdsByCollection = keys.reduce((acc, key) => {
      const collectionName = key.collectionName || nonCollectionName;
      const fileIds = acc[collectionName] || [];

      // de-duplicate ids in collection
      if (fileIds.indexOf(key.id) === -1) {
        fileIds.push(key.id);
      }
      acc[collectionName] = fileIds;
      return acc;
    }, {});
    const items = [];
    await Promise.all(Object.keys(fileIdsByCollection).map(async collectionNameKey => {
      const metadataTraceContext = {
        traceId: getRandomHex(8),
        spanId: getRandomHex(8)
      };
      const fileIds = fileIdsByCollection[collectionNameKey];
      const includeHashForDuplicateFiles = includeHashByCollection[collectionNameKey];
      const collectionName = collectionNameKey === nonCollectionName ? undefined : collectionNameKey;
      try {
        const response = await mediaStore.getItems(fileIds, collectionName, metadataTraceContext, includeHashForDuplicateFiles);
        const itemsWithMetadataTraceContext = response.data.items.map(item => ({
          ...item,
          metadataTraceContext
        }));
        items.push(...itemsWithMetadataTraceContext);

        // add EmptyResponseFileItem for each file ID not included in /items response
        const itemsIds = itemsWithMetadataTraceContext.map(item => item.id);
        const fileIdsNotFound = fileIds.filter(id => !itemsIds.includes(id));
        fileIdsNotFound.forEach(fileId => {
          items.push({
            id: fileId,
            collection: collectionName,
            type: 'not-found',
            metadataTraceContext
          });
        });
      } catch (error) {
        fileIds.forEach(fileId => {
          items.push({
            id: fileId,
            collection: collectionName,
            error: error
          });
        });
      }
    }));
    return getItemsFromKeys(keys, items);
  };
}
export function createFileDataloader(mediaStore) {
  return new Dataloader(createBatchLoadingFunc(mediaStore), {
    maxBatchSize: MAX_BATCH_SIZE
  });
}